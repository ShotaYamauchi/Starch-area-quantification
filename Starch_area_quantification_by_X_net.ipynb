{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Starch Area Quantification by X-net**\n",
        "\n",
        "This Colab notebook provides a pipeline for quantifying starch granule areas in stomatal guard cells, as visualized by PS-PI staining. We utilized X-net for automated starch segmentation; comprehensive details regarding the model training and architecture are provided in methods section.\n",
        "\n",
        "**Note on Model Generalizability:**\n",
        "This model was optimized for images acquired with a Nikon C1 confocal microscope (100x objective, $50.54 \\times 50.54$ $\\mu\\text{m}$ field of view). When applying this pipeline to images from different optical setups, we strongly recommend validating the automated results against manual analysis in ImageJ, following the protocol established by Flütsch et al. (2018), to ensure quantitative accuracy.\n",
        "\n",
        "**Reference**\n",
        "Flütsch S., Distefano L., and Santelia D. Quantification of starch in guard cells of Arabidopsis thaliana. Bio-protocol 8, e2920 (2018).\n"
      ],
      "metadata": {
        "id": "xy8e-PccZxHz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxFQkHI8wlqL"
      },
      "source": [
        "**Preparation for Analysis**\n",
        "\n",
        "1, Environment Setup:\n",
        "Ensure that your trained model file (model.pth) and the folder containing your confocal images are uploaded to your Google Drive.\n",
        "\n",
        "2, Drive Mounting and Authentication:\n",
        "Execute the first two code cells. You will be prompted to authenticate your Google account and grant the necessary permissions.\n",
        "\n",
        "3, Accessing Data:\n",
        "Once authenticated, the Google Drive directory (/content/drive/) will appear in the \"Files\" tab on the left sidebar. This allows the notebook to directly read your input data and save the analysis results back to your Drive."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U --no-cache-dir imagecodecs tifffile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lluYr1SDRjnT",
        "outputId": "cfd594ee-e322-4e18-9a56-29725df69be5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting imagecodecs\n",
            "  Downloading imagecodecs-2026.1.14-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.12/dist-packages (2026.1.28)\n",
            "Collecting tifffile\n",
            "  Downloading tifffile-2026.2.16-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from imagecodecs) (2.0.2)\n",
            "Downloading imagecodecs-2026.1.14-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (24.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m135.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tifffile-2026.2.16-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m257.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tifffile, imagecodecs\n",
            "  Attempting uninstall: tifffile\n",
            "    Found existing installation: tifffile 2026.1.28\n",
            "    Uninstalling tifffile-2026.1.28:\n",
            "      Successfully uninstalled tifffile-2026.1.28\n",
            "Successfully installed imagecodecs-2026.1.14 tifffile-2026.2.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBt6SD3dq5ly",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "302a7e84-6276-45fd-aefe-79b61f2778dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tGOhcZsmKeb"
      },
      "source": [
        "Import libraries\n",
        "\n",
        "Please run the code below to import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsUPyoFLhVcY"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch.utils.data as data\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import random\n",
        "import numbers\n",
        "import numpy as np\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import seaborn as sns\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm\n",
        "from natsort import natsorted\n",
        "from PIL import Image\n",
        "from enum import Enum\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR6aj64AiOWo"
      },
      "source": [
        "Definition of DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4FH4UKsGVLz"
      },
      "outputs": [],
      "source": [
        "# compose\n",
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, img, lbl):\n",
        "        for t in self.transforms:\n",
        "            img, lbl = t(img, lbl)\n",
        "        return img, lbl\n",
        "\n",
        "# to Tensor\n",
        "class ToTensor(object):\n",
        "    def __init__(self, normalize=True, target_type='uint8'):\n",
        "        self.normalize = normalize\n",
        "        self.target_type = target_type\n",
        "    def __call__(self, pic, lbl):\n",
        "        if self.normalize:\n",
        "            return TF.to_tensor(pic), torch.from_numpy( np.array( lbl, dtype=self.target_type) )\n",
        "        else:\n",
        "            return torch.from_numpy( np.array( pic, dtype=np.float32).transpose(2, 0, 1) ), torch.from_numpy( np.array( lbl, dtype=self.target_type) )\n",
        "\n",
        "class DataLoader(data.Dataset):\n",
        "\n",
        "    def __init__(self, img_path, transform=None):\n",
        "\n",
        "        self.image_path = img_path\n",
        "\n",
        "        self.image_list = sorted(os.listdir(self.image_path))\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        image_name = self.image_list[index]\n",
        "\n",
        "\n",
        "        image = Image.open(self.image_path + \"/{}\".format(image_name)).convert(\"RGB\")\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "\n",
        "            image, label = self.transform(image,image)\n",
        "\n",
        "        return image, image_name\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.image_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJH_wkHjiYxO"
      },
      "source": [
        "Definition of Neural Network model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmoXSYmjGqyL"
      },
      "outputs": [],
      "source": [
        "class ChannelSELayer(nn.Module):\n",
        "    def __init__(self, num_channels, reduction_ratio=4):\n",
        "        super(ChannelSELayer, self).__init__()\n",
        "        num_channels_reduced = num_channels // reduction_ratio\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "        self.fc1 = nn.Linear(num_channels, num_channels_reduced, bias=True)\n",
        "        self.fc2 = nn.Linear(num_channels_reduced, num_channels, bias=True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        batch_size, num_channels, H, W = input_tensor.size()\n",
        "        # Average along each channel\n",
        "        squeeze_tensor = input_tensor.view(batch_size, num_channels, -1).mean(dim=2)\n",
        "\n",
        "        # channel excitation\n",
        "        fc_out_1 = self.relu(self.fc1(squeeze_tensor))\n",
        "        fc_out_2 = self.sigmoid(self.fc2(fc_out_1))\n",
        "\n",
        "        a, b = squeeze_tensor.size()\n",
        "        output_tensor = torch.mul(input_tensor, fc_out_2.view(a, b, 1, 1))\n",
        "        return output_tensor\n",
        "\n",
        "\n",
        "class SpatialSELayer(nn.Module):\n",
        "    def __init__(self, num_channels):\n",
        "        super(SpatialSELayer, self).__init__()\n",
        "        self.conv = nn.Conv2d(num_channels, 1, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_tensor, weights=None):\n",
        "        # spatial squeeze\n",
        "        batch_size, channel, a, b = input_tensor.size()\n",
        "\n",
        "        if weights:\n",
        "            weights = weights.view(1, channel, 1, 1)\n",
        "            out = F.conv2d(input_tensor, weights)\n",
        "        else:\n",
        "            out = self.conv(input_tensor)\n",
        "        squeeze_tensor = self.sigmoid(out)\n",
        "\n",
        "        # spatial excitation\n",
        "        output_tensor = torch.mul(input_tensor, squeeze_tensor.view(batch_size, 1, a, b))\n",
        "\n",
        "        return output_tensor\n",
        "\n",
        "\n",
        "class ChannelSpatialSELayer(nn.Module):\n",
        "    def __init__(self, num_channels, reduction_ratio=16):\n",
        "        super(ChannelSpatialSELayer, self).__init__()\n",
        "        self.cSE = ChannelSELayer(num_channels, reduction_ratio)\n",
        "        self.sSE = SpatialSELayer(num_channels)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output_tensor = self.cSE(input_tensor) + self.sSE(input_tensor)\n",
        "        return output_tensor\n",
        "\n",
        "\n",
        "class BaseBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(BaseBlock, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_ch, in_ch, 3, 1, 1), nn.BatchNorm2d(in_ch), nn.ReLU())\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_ch, in_ch, 3, 1, 1), nn.BatchNorm2d(in_ch), nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, ch=64):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.block = BaseBlock(in_ch=ch, out_ch=ch)\n",
        "        self.conv = nn.Sequential(nn.Conv2d(ch, ch*2, 3, 1, 1), nn.BatchNorm2d(ch*2), nn.ReLU())\n",
        "        self.se = ChannelSpatialSELayer(num_channels=ch*2)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.se(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, ch=1024, dilation=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.deconv1 = nn.ConvTranspose2d(ch, ch//4, 3, 2, 1, dilation)\n",
        "        self.block = BaseBlock(in_ch=ch//4, out_ch=ch//4)\n",
        "        self.se = ChannelSpatialSELayer(num_channels=ch//4)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x = torch.cat((x1,x2),1)\n",
        "        x = self.deconv1(x)\n",
        "        x = self.block(x)\n",
        "        x = self.se(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class XNet(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, ch=64, dil=1):\n",
        "        super(XNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, ch, 3, 1, 1, dilation=dil)\n",
        "        self.bn1 = nn.BatchNorm2d(ch)\n",
        "\n",
        "        self.enc1_1 = Encoder(ch)\n",
        "        self.enc1_2 = Encoder(ch*2)\n",
        "        self.enc1_3 = Encoder(ch*4)\n",
        "        self.dec1_1 = Decoder(1536, dilation=dil)\n",
        "        self.dec1_2 = Decoder(640, dilation=dil)\n",
        "        self.dec1_3 = Decoder(288, dilation=dil)\n",
        "\n",
        "        self.center_conv = BaseBlock(in_ch=ch*8*2, out_ch=ch*8*2)\n",
        "\n",
        "        self.enc2_1 = Encoder(ch)\n",
        "        self.enc2_2 = Encoder(ch*2)\n",
        "        self.enc2_3 = Encoder(ch*4)\n",
        "        self.dec2_1 = Decoder(1536, dilation=dil)\n",
        "        self.dec2_2 = Decoder(640, dilation=dil)\n",
        "        self.dec2_3 = Decoder(288, dilation=dil)\n",
        "\n",
        "        self.out1_conv = nn.Conv2d(72, out_ch, 1, 1)\n",
        "        self.out2_conv = nn.Conv2d(72, out_ch, 1, 1)\n",
        "        self.out3_conv = nn.Conv2d(144, out_ch, 1, 1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        h = F.relu(self.bn1(self.conv1(x)))\n",
        "        h1 = h\n",
        "        h2 = h\n",
        "\n",
        "        h1_e1 = self.enc1_1(h1)\n",
        "        h2_e1 = self.enc2_1(h2)\n",
        "\n",
        "        h1_e2 = self.enc1_2(h1_e1)\n",
        "        h2_e2 = self.enc2_2(h2_e1)\n",
        "\n",
        "        h1_e3 = self.enc1_3(h1_e2)\n",
        "        h2_e3 = self.enc2_3(h2_e2)\n",
        "\n",
        "        cat = torch.cat((h1_e3,h2_e3),1)\n",
        "        hc = self.center_conv(cat)\n",
        "\n",
        "        h1_d1 = self.dec1_1(hc,h1_e3)\n",
        "        h2_d1 = self.dec2_1(hc, h2_e3)\n",
        "\n",
        "        h1_d2 = self.dec1_2(h1_d1,h1_e2)\n",
        "        h2_d2 = self.dec2_2(h2_d1, h2_e2)\n",
        "\n",
        "        h1_d3 = self.dec1_3(h1_e1,h1_d2)\n",
        "        h2_d3 = self.dec2_3(h2_e1,h2_d2)\n",
        "        cat = torch.cat((h1_d3, h2_d3),1)\n",
        "\n",
        "        out1 = self.out1_conv(h1_d3)\n",
        "        out2 = self.out2_conv(h2_d3)\n",
        "        out3 = self.out3_conv(cat)\n",
        "\n",
        "        return out1, out2, out3, cat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aImZClEzzoWN",
        "outputId": "2a06dd12-2f25-4e72-bf76-187e30361b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw6pRFOyggH3"
      },
      "source": [
        " ↓ ① Specify the image folder path (Update this for each analysis)\n",
        "\n",
        "In the code cell below, replace the empty brackets in list = [] with the path to the folder containing the images you wish to analyze.\n",
        "\n",
        "Follow these steps to copy the folder path:\n",
        "\n",
        "1, Click the Files icon in the left sidebar.\n",
        "\n",
        "2, Navigate to drive > MyDrive.\n",
        "\n",
        "3, Right-click the folder containing your images.\n",
        "\n",
        "4, Select \"Copy path\".\n",
        "\n",
        "5, Paste the path inside the brackets of list = [].\n",
        "\n",
        "Example:\n",
        "If your image file is located at:\n",
        "/content/drive/MyDrive/Analysis Images/Data A/img_a1.tif\n",
        "You must specify the path up to the folder level (\"Data A\"):\n",
        "list = [\"/content/drive/MyDrive/Analysis Images/Data A\"]\n",
        "\n",
        "Note:\n",
        "\n",
        "Enclose each path in quotation marks (\"\").\n",
        "\n",
        "If you enter multiple paths, separate them with commas.\n",
        "\n",
        "Example: list = [\"path_A\", \"path_B\", \"path_C\"]\n",
        "\n",
        "(You may add line breaks after each comma for better readability.)\n",
        "\n",
        "Please ensure you re-run this cell whenever you change the path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9mhJDjrE_SL"
      },
      "outputs": [],
      "source": [
        "#①Specify the path where the image exists\n",
        "def path_list():\n",
        "  #Replace the path where the images to be analyzed exists\n",
        "    list = [\n",
        "        \"/content/drive/MyDrive/XXX\",\n",
        "\n",
        "\n",
        "    ]\n",
        "\n",
        "    return list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m939-TDa2FCv"
      },
      "source": [
        "# ↓ ② Preprocessing: Crop images to 512 x 512\n",
        "\n",
        "This step crops the images to 512 x 512 pixels. The cropped images will be saved in a new folder named crop_img created within each source directory.\n",
        "\n",
        "During execution, the path of the image currently being processed and the progress status will be displayed in the output area below the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQUz2LnO184j"
      },
      "outputs": [],
      "source": [
        "#②Crop images to 512x512 as a preprocessing step\n",
        "folda_path = path_list()\n",
        "\n",
        "for i in folda_path:\n",
        "    print(i)\n",
        "    if os.path.exists(i)==False:\n",
        "            print(\"######################################################################\")\n",
        "            print(\"No file\",i)#It is indicated if the path is wrong\n",
        "            print(\"######################################################################\")\n",
        "\n",
        "\n",
        "    if os.path.exists(\"{}/{}\".format(i, \"crop_img\"))==False:\n",
        "        os.makedirs(\"{}/{}\".format(i, \"crop_img\"))\n",
        "\n",
        "    imgs = glob.glob(i + \"/*.tif\")\n",
        "\n",
        "    for j in tqdm(imgs):\n",
        "\n",
        "        img = cv2.imread(j)\n",
        "\n",
        "        file_name = j.split(\"/\")[-1]\n",
        "\n",
        "        h, w, c = img.shape\n",
        "\n",
        "        if h < 512:\n",
        "            space = 512-h\n",
        "            img = cv2.copyMakeBorder(img, 0, space, 0, 0, cv2.BORDER_CONSTANT, value=(0,0,0))\n",
        "        if w < 512:\n",
        "            space = 512-w\n",
        "            img = cv2.copyMakeBorder(img, 0, 0, 0, space, cv2.BORDER_CONSTANT, value=(0,0,0))\n",
        "\n",
        "\n",
        "        crop = img[0 : 512, 0 : 512]\n",
        "\n",
        "        cv2.imwrite(\"{}/{}/{}\".format(i, \"crop_img\", file_name), crop)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0GGdT9G2TPb"
      },
      "source": [
        " # ↓ ③ Segmentation: Identify starch granules (Update model path for each analysis)\n",
        "\n",
        "In the code cell below, replace the default path model_path = \"model.pth\" with the actual file path where your model is stored.\n",
        "\n",
        "Example:\n",
        "model_path = \"/content/drive/MyDrive/model.pth\"\n",
        "\n",
        "How to set the model path:\n",
        "\n",
        "1, Click the Files icon in the left sidebar.\n",
        "\n",
        "2, Navigate to drive > MyDrive.\n",
        "\n",
        "3, Locate the model file (model.pth) and right-click it.\n",
        "\n",
        "4, Select \"Copy path\".\n",
        "\n",
        "5, Paste the path between the quotation marks in model_path = \"\".\n",
        "(Note: The path must be enclosed in quotation marks.)\n",
        "\n",
        "Output:\n",
        "Upon execution, two new folders named segmentation and feature_map will be created within the source directory. The resulting segmentation images and visualization maps will be saved in their respective folders.\n",
        "\n",
        "Progress:\n",
        "During execution, the path of the image currently being processed and the progress status will be displayed in the output area below the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liCcBElM2M13"
      },
      "outputs": [],
      "source": [
        "#③A program to conduct segmentation\n",
        "from tifffile import imread\n",
        "import imagecodecs # Add this line to explicitly import imagecodecs\n",
        "\n",
        "# compose\n",
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, img, lbl):\n",
        "        for t in self.transforms:\n",
        "            img, lbl = t(img, lbl)\n",
        "        return img, lbl\n",
        "\n",
        "# to Tensor\n",
        "class ToTensor(object):\n",
        "    def __init__(self, normalize=True, target_type='uint8'):\n",
        "        self.normalize = normalize\n",
        "        self.target_type = target_type\n",
        "    def __call__(self, pic, lbl):\n",
        "        if self.normalize:\n",
        "            return TF.to_tensor(pic), torch.from_numpy( np.array( lbl, dtype=self.target_type) )\n",
        "        else:\n",
        "            return torch.from_numpy( np.array( pic, dtype=np.float32).transpose(2, 0, 1) ), torch.from_numpy( np.array( lbl, dtype=self.target_type) )\n",
        "\n",
        "class DataLoader(data.Dataset):\n",
        "\n",
        "    def __init__(self, img_path, transform=None):\n",
        "\n",
        "        self.image_path = img_path\n",
        "\n",
        "        self.image_list = sorted(os.listdir(self.image_path))\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        image_name = self.image_list[index]\n",
        "\n",
        "\n",
        "        image = Image.open(self.image_path + \"/{}\".format(image_name)).convert(\"RGB\")\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "\n",
        "            image, label = self.transform(image,image)\n",
        "\n",
        "        return image, image_name\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.image_list)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def dataload(path):\n",
        "    test_transform = Compose([ToTensor(),\n",
        "                                 ])\n",
        "    test_dataset = DataLoader(img_path = path, transform=test_transform)\n",
        "\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, drop_last=True)\n",
        "\n",
        "    return test_loader\n",
        "\n",
        "\n",
        "\n",
        "def Save_fmap(x, img_name):\n",
        "    x = x.mean(dim=1)\n",
        "    x = x.cpu().numpy()\n",
        "    x = np.squeeze(x)\n",
        "    x = min_max(x)\n",
        "    x = x[np.newaxis,:,:]\n",
        "    x = x[0]\n",
        "    x = cv2.applyColorMap(np.uint8(x*255.0), cv2.COLORMAP_JET)\n",
        "    cv2.imwrite(\"{}/{}/map_{}.png\".format(i, \"feature_map\", img_name), x)\n",
        "\n",
        "def Save_image(input_img, output, img_name):\n",
        "\n",
        "    output = np.argmax(output,axis=1)\n",
        "    output = output[0]\n",
        "    out_array = np.zeros((image_size[1],image_size[2],3))\n",
        "    out_array[output==0] = [1.0, 0.0, 0.0]\n",
        "    out_array[output==1] = [0.0, 0.0, 0.0]\n",
        "    out_array = cv2.cvtColor(np.uint8(out_array*255.0), cv2.COLOR_BGR2RGB)\n",
        "    cv2.imwrite(\"{}/{}/seg_{}.png\".format(i, \"segmentation\", img_name), out_array)\n",
        "\n",
        "\n",
        "def min_max(x, axis=None):\n",
        "    min = x.min(axis=axis, keepdims=True)\n",
        "    max = x.max(axis=axis, keepdims=True)\n",
        "    result = (x-min)/(max-min)\n",
        "    return result\n",
        "\n",
        "\n",
        "def test():\n",
        "    #Replace model.pth\n",
        "    model_path = \"/content/drive/MyDrive/model.pth\"\n",
        "\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (inputs, img_name) in enumerate(tqdm(test_loader)):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        inputs = inputs.to(device, non_blocking=True)\n",
        "\n",
        "\n",
        "        inputs = Variable(inputs)\n",
        "\n",
        "\n",
        "        out1, out2, out3, y  = model(inputs)\n",
        "\n",
        "\n",
        "        output = F.softmax(out3, dim=1)\n",
        "        output = output.cpu().numpy()\n",
        "        inputs = inputs.cpu().numpy()\n",
        "\n",
        "        img_name = ''.join(img_name)\n",
        "        img_name = img_name.replace(\".tif\", \"\")\n",
        "\n",
        "\n",
        "        Save_image(inputs, output, img_name)\n",
        "        Save_fmap(y, img_name)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    folda_path = path_list()\n",
        "    for i in folda_path:\n",
        "        print(i)\n",
        "\n",
        "\n",
        "        if os.path.exists(\"{}/{}\".format(i, \"segmentation\"))==False:\n",
        "            os.makedirs(\"{}/{}\".format(i, \"segmentation\"))\n",
        "        if os.path.exists(\"{}/{}\".format(i, \"feature_map\"))==False:\n",
        "            os.makedirs(\"{}/{}\".format(i, \"feature_map\"))\n",
        "\n",
        "\n",
        "        crop_img_path = \"{}/{}\".format(i, \"crop_img\")\n",
        "        test_loader = dataload(crop_img_path)\n",
        "\n",
        "\n",
        "        image_size = [3, 512, 512]\n",
        "        label_list = [\"Starch\", \"Background\"]\n",
        "\n",
        "\n",
        "        model = XNet(in_ch=image_size[0], out_ch=len(label_list))\n",
        "        model = model.to(device)\n",
        "\n",
        "\n",
        "        test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuq7j_Hp2jX6"
      },
      "source": [
        "# ↓ ④ Quantification of starch granule area\n",
        "\n",
        "Upon execution, the program creates a new folder named labeling within the source directory. The analysis results, including images and text files, will be stored in this folder.\n",
        "\n",
        "During execution, the path of the image currently being processed and the progress status will be displayed in the output area below the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXWvYk9s2bk5"
      },
      "outputs": [],
      "source": [
        "#④ To quantify starch granule area from segmantation image\n",
        "\n",
        "def Labeling(img_path, img_list):\n",
        "    labeling_result = \"{}/{}/{}.txt\".format(img_path, \"labeling\", \"labeling_result\")\n",
        "    with open(labeling_result, mode = 'w') as f:\n",
        "        pass\n",
        "    for i, path in enumerate(tqdm(img_list)):\n",
        "        img_name = path.split(\"/\")[-1].replace(\"seg_\", \"\").replace(\".png\", \"\")\n",
        "        with open(labeling_result, mode = 'a') as f:\n",
        "            f.write(\"\\n%s\\n\" % (img_name))\n",
        "\n",
        "        img = cv2.imread(path)\n",
        "\n",
        "\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "\n",
        "        gray = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
        "\n",
        "\n",
        "        label = cv2.connectedComponentsWithStats(gray)\n",
        "\n",
        "\n",
        "        n = label[0] - 1\n",
        "        data = np.delete(label[2], 0, 0)\n",
        "        center = np.delete(label[3], 0, 0)\n",
        "\n",
        "\n",
        "        color_src = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "        sum_s = 0\n",
        "\n",
        "        for j in range(n):\n",
        "\n",
        "            x0 = data[j][0]\n",
        "            y0 = data[j][1]\n",
        "            x1 = data[j][0] + data[j][2]\n",
        "            y1 = data[j][1] + data[j][3]\n",
        "            cv2.rectangle(color_src, (x0, y0), (x1, y1), (0, 255, 255))\n",
        "\n",
        "\n",
        "            cv2.putText(color_src,\n",
        "                        str(j + 1),\n",
        "                        (x0, y1 + 8),\n",
        "                        cv2.FONT_HERSHEY_PLAIN,\n",
        "                        0.5,\n",
        "                        (0, 0, 255)\n",
        "                        )\n",
        "            cv2.putText(color_src,\n",
        "                        \"ID:{} S: \".format(j+1) +str(data[j][4]),\n",
        "                        (0, 10*(j+1)),\n",
        "                        cv2.FONT_HERSHEY_PLAIN,\n",
        "                        0.7,\n",
        "                        (0, 0, 255)\n",
        "                        )\n",
        "            sum_s += data[j][4]\n",
        "            with open(labeling_result, mode = 'a') as f:\n",
        "                f.write(\"ID:%d\\t%d\\n\" % (j+1, data[j][4]))\n",
        "\n",
        "\n",
        "        with open(labeling_result, mode = 'a') as f:\n",
        "                f.write(\"Total:\\t%d\\n\" % (sum_s))\n",
        "\n",
        "        cv2.imwrite(\"{}/{}/labeling_{}.png\".format(img_path, \"labeling\", img_name), color_src)\n",
        "\n",
        "folda_path = path_list()\n",
        "for i in folda_path:\n",
        "    print(i)\n",
        "\n",
        "\n",
        "    if os.path.exists(\"{}/{}\".format(i, \"labeling\"))==False:\n",
        "        os.makedirs(\"{}/{}\".format(i, \"labeling\"))\n",
        "\n",
        "    seg_img_path = \"{}/{}\".format(i, \"segmentation\")\n",
        "    seg_img_list = natsorted(glob.glob(seg_img_path + \"/*.png\"))\n",
        "\n",
        "    Labeling(i, seg_img_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Calculation of starch granule area**\n",
        "\n",
        "The file labeling_result.txt is located in the labeling folder. When you open this file in Microsoft Excel:\n",
        "\n",
        "Column A displays the ID numbers of the starch granules identified by segmentation.\n",
        "\n",
        "Column B shows the calculated area (in pixels) of each granule.\n",
        "\n",
        "The value in Column B, corresponding to the \"total\" label in Column A, represents the sum of the starch granule areas within the guard cells.\n",
        "To determine the actual area in square micrometers ($\\mu m^2$), divide the pixel values obtained from the labeling results by the pixel density (pixels/$\\mu m^2$) of the analyzed image.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "Multiple Stomata: If an image contains multiple stomata, starch granules from all stomata will be detected and included in the total.\n",
        "\n",
        "False Positives: Background fluorescence or starch granules in mesophyll cells may be inadvertently detected. Please review the segmentation and feature_map images to verify the results and avoid erroneous measurements.\n"
      ],
      "metadata": {
        "id": "11BKsl1ta_zm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you hava any question please contact to Dr. Atsushi Takemiya (take.pcs at yamaguchi-u.ac.jp), Dr. Kazuhiro Hotta(kazuhotta at meijo-u.ac.jp), Dr. Shota Yamauchi (shyamauchi at rs.tus.ac.jp)"
      ],
      "metadata": {
        "id": "KeR9NunMYvKG"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}